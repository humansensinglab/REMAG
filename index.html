<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="">
  <meta name="keywords" content="synthetic data, aerial imagery, human action recognition, human activity recognition, action recognition, activity recognition, dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Exploring the Impact of Rendering Method and Motion Quality on Model Performance when Using Multi-view Synthetic Data for Action Recognition</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>

  <link rel="shortcut icon" href="./assets/block.png">


</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Exploring the Impact of Rendering Method and Motion Quality on Model Performance when Using a Multi-view Synthetic Data for Action Recognition</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
			  <a href="https://www.linkedin.com/in/stanislav-panev/">Stanislav Panev</a><sup>*1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <a href="https://kimemily12.github.io/website/" target="_blank">Emily Kim</a><sup>*1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <a href="https://namburusiddhartha.github.io/" target="_blank">Sai Abhishek Si Namburu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <a href="https://www.linkedin.com/in/desislava-v-nikolova/">Desislava Nikolova</a><sup>2</sup><br/>
			  <a href="https://celsodemelo.net/" target="_blank">Celso de Melo</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <a href="https://www.cs.cmu.edu/~ftorre/" target="_blank">Fernando de la Torre</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <a href="https://www.cs.cmu.edu/~jkh/" target="_blank">Jessica Hodgins</a><sup>1</sup>
			   
              <br>
			  <sup>1</sup>Carnegie Mellon University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <sup>2</sup>Technical University of Sofia&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			  <sup>3</sup>Army Research Lab<br>
              <br>
              <h2><strong>WACV 2024</strong></h2>
          </div>
		  
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- ArXiv Link. -->
			  <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv - Coming soon</span>
                </a>
              </span> -->
			  
			  <!-- CVF OpenAccess Link. -->
			  <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/WACV2024/html/Panev_Exploring_the_Impact_of_Rendering_Method_and_Motion_Quality_on_WACV_2024_paper.html"
                   class="external-link button is-normal is-rounded is-dark"
				   target="_blank">
                  <span class="icon">
                      <i class="fa-solid fa-newspaper"></i>
                  </span>
				  <span>Paper</span>
                </a>
              </span>
				
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=hDlpJMyibBQ"
                   class="external-link button is-normal is-rounded is-dark"
				   target="_blank">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
			  
			  <!-- Datasets Link. -->
              <span class="link-block">
                <a href="#Datasets"
                   class="button is-normal is-rounded is-dark">
                  <span class="icon">
					  <i class="fa-solid fa-database"></i>
					  
                  </span>
                  <span>Datasets</span>
                </a>
              </span>
			  
			  <!-- BibTeX Link. -->
              <span class="link-block">
                <a href="#BibTex"
                   class="button is-normal is-rounded is-dark">
                  <span class="icon">
					<i class="fa-solid fa-quote-left"></i>
					  <!-- <i class="fa-solid fa-newspaper"></i> -->
					  <!-- <img src="assets/BibTeX_logo.svg" width="60px"> -->
                  </span>
				  <span>BibTeX</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <center><img style="width: 100%;" src="./assets/images/Teaser.png"></center>
      <h2 class="subtitle has-text-centered">
        REMAG - an HAR dataset suite comprises five datasets: one real and four synthetic by combining two renderers (CG and neural) with two motion sources (motion capture and video-based). Each of them includes three camera views. 
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
			This paper explores the use of synthetic data in a human action recognition (HAR) task to avoid the challenges of obtaining and labeling real-world datasets.
			We introduce a new dataset suite comprising five datasets, eleven common human activities, three synchronized camera views (aerial and ground) in three outdoor environments, and three visual domains (real and two synthetic). 
			For the synthetic data, two rendering methods (standard computer graphics and neural rendering) and two sources of human motions (motion capture and video-based motion reconstruction) were employed.
			We evaluated each dataset type by training popular activity recognition models and comparing the performance on the real test data.
			Our results show that synthetic data achieve slightly lower accuracy (4-8%) than real data.
			On the other hand, a model pre-trained on synthetic data and fine-tuned on limited real data surpasses the performance of either domain alone.
			Standard computer graphics (CG)-rendered data delivers better performance than the data generated from the neural-based rendering method.
			The results suggest that the quality of the human motions in the training data also affects the test results: motion capture delivers higher test accuracy.
			Additionally, a model trained on CG aerial view synthetic data exhibits greater robustness against camera viewpoint changes than one trained on real data.
          </p>
        </div>
      </div>
    </div>
  </div>
  <!--/ Abstract. -->  
</section>


<section class="section" id="Datasets">
  <div class="container content">
    <h2 class="title">Datasets</h2>
	<div class="content">
		<p>Download links for 11-activity datasets (ZIP files, Google Drive):</p>
		<ul>
			<li>
				Real Dataset
				<ul>
					<li><a href="https://drive.google.com/file/d/1TRo7LPWwDEqP3i5uSGg7UHl8NFsIq1gb/view?usp=sharing" target="_blank">Aerial View <i class="fa-solid fa-up-right-from-square"></i></a> (MD5: cadb0d24d595cdca65ba4d16c6c14109)</li>
					<li><a href="https://drive.google.com/file/d/1Q6fCuEeg5Dghcei7ESck3DdzWMF1qV_-/view?usp=sharing" target="_blank">Ground View <i class="fa-solid fa-up-right-from-square"></i></a> (MD5: 445e66b91ecf68390f4505bebee382b2)</li>
				</ul>
			</li>
			<li>
				Synthetic Datasets
				<ul>
					<li>
						[SynCG-MC] Computer Graphics + Motion capture motions
						<ul>
							<li><a href="https://drive.google.com/file/d/15vQD-8d8cPTcwVgwM2WX1N9iA3Fk0yHS/view?usp=sharing" target="_blank">Aerial View <i class="fa-solid fa-up-right-from-square"></i></a> (MD5: a7eec0f4576242d188e74ee10ebc877e)</li>
							<li><a href="https://drive.google.com/file/d/1QVuQdshKgJ4gTqH4fvcLK-wqN9dC2pNZ/view?usp=sharing" target="_blank">Ground View <i class="fa-solid fa-up-right-from-square"></i></a> (MD5: ca2955cb44a3ac3c074e53406745f41c)</li>
						</ul>
					</li>
					<li>
						[SynCG-RGB] Computer Graphics + Video-based motions (Link)
						<ul>
							<li><a href="https://drive.google.com/file/d/16oeW9MxQBDpi3lr7BEi_VhQImSL1q7s2/view?usp=sharing" target="_blank">Aerial View <i class="fa-solid fa-up-right-from-square"></i></a> (MD5: f5657e247de7b74a83ab4df79e1b33e5)</li>
							<li><a href="https://drive.google.com/file/d/1_ojiJLdtM14GqqtyuqKorejZLkDyBHCS/view?usp=sharing" target="_blank">Ground View <i class="fa-solid fa-up-right-from-square"></i></a> (MD5: 1cd58dc521c532eaf7a994bffdef62ff)</li>
						</ul>
					</li>
					<li>
						[SynLWG-MC] Neural rendering (Liquid Warping GAN) + Motion capture motions (Link)
						<ul>
							<li><a href="https://drive.google.com/file/d/10TzQYxuqmXQR_gKM2rkJuVnCYJrv9nxO/view?usp=sharing" target="_blank">Aerial View <i class="fa-solid fa-up-right-from-square"></i></a> (MD5: 3a8ae0c7a539a4e52ff3e112fe9f9af9)</li>
							<li><a href="https://drive.google.com/file/d/1D56piluEU88fjsl2ILo-GQUeaa_MWIjE/view?usp=sharing" target="_blank">Ground View <i class="fa-solid fa-up-right-from-square"></i></a> (MD5: 68a3cdb8019b08fff3cb7d2b9bc05576)</li>
						</ul>
					</li>
					<li>
						[SynLWG-RGB] Neural rendering (Liquid Warping GAN) + Video-based motions (Link)
						<ul>
							<li><a href="https://drive.google.com/file/d/1Zn0BjYu2GFEVzjisDFjsuxClIMzW6dH-/view?usp=sharing" target="_blank">Aerial View <i class="fa-solid fa-up-right-from-square"></i></a> (MD5: f04b912914478515bdbebec84b27b901)</li>
							<li><a href="https://drive.google.com/file/d/1wTYpTtunDTw-OR3gNrf-vUptT_Tr1NjS/view?usp=sharing" target="_blank">Ground View <i class="fa-solid fa-up-right-from-square"></i></a> (MD5: c00dfd92321415b42704bb2e4b6d9cbe)</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</div>
  </div>
</section>

<section class="section" id="License">
  <div class="container content">
    <h2 class="title">License</h2>
	<div class="content">
		<p>The datasets are released under <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank"><b>Creative Commons Attribution 4.0 International (CC BY 4.0)</b></a> license.</p>
		<p><i class="fab fa-creative-commons"></i><i class="fa-brands fa-creative-commons-by"></i></p>
	</div>
  </div>
</section>

<section class="section" id="BibTex">
  <div class="container content">
    <h2 class="title">BibTex</h2>
    <pre><code>@InProceedings{Panev_2024_WACV,
	author    = {Panev, Stanislav and Kim, Emily and Namburu, Sai Abhishek Si and Nikolova, Desislava and de Melo, Celso and De la Torre, Fernando and Hodgins, Jessica},
	title     = {Exploring the Impact of Rendering Method and Motion Quality on Model Performance When Using Multi-View Synthetic Data for Action Recognition},
	booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
	month     = {January},
	year      = {2024},
	pages     = {4592-4602}
}</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>Page template borrowed from <a href="https://nerfies.github.io/"><span class="dnerf">Nerfies</span></a>.</p>
      <p>Copyright © 2023 Carnegie Mellon University</p>
   </div>
  </div>
</footer>
  
</body>
</html>
